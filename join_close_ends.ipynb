{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162c7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from typing import Any\n",
    "from topostats.io import LoadScans\n",
    "from topostats.plottingfuncs import Colormap\n",
    "from topostats.utils import convolve_skeleton\n",
    "from topostats.mask_manipulation import smooth_mask\n",
    "from topostats.tracing.skeletonize import getSkeleton\n",
    "from topostats.measure.geometry import calculate_mask_width_with_skeleton\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colormap = Colormap()\n",
    "cmap = colormap.get_cmap()\n",
    "vmin = -3.0\n",
    "vmax = 4.0\n",
    "\n",
    "\n",
    "def clear_output():\n",
    "    from IPython.display import clear_output as co\n",
    "    co()\n",
    "\n",
    "\n",
    "def load_data(dir: Path) -> dict[str, Any]:\n",
    "    files = list(dir.glob(\"*.topostats\"))\n",
    "    loader = LoadScans(files, channel=\"dummy\")\n",
    "    loader.get_data()\n",
    "    clear_output()\n",
    "    return loader.img_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_mid_process_files = Path(\"/Users/sylvi/topo_data/connect-loose-ends/mid-topostats-processing-data-files\")\n",
    "files = list(dir_mid_process_files.glob(\"*.pkl\"))\n",
    "loaded_files: dict[str, dict] = {}\n",
    "for file in files:\n",
    "    with open(file, \"rb\") as f:\n",
    "        loaded_file = pickle.load(f)\n",
    "        filename = loaded_file[\"filename\"]\n",
    "        loaded_files[filename] = loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56e9b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_nonrepeated_endpoints(\n",
    "    potential_pairs: list[list[list[int, int], list[int, int], float]],\n",
    ") -> list[list[list[int, int], list[int, int], float]]:\n",
    "\n",
    "    used_endpoints: list[tuple[int, int]] = []\n",
    "    for potential_pair in potential_pairs:\n",
    "        endpoint_1, endpoint_2, distance_nm = potential_pair\n",
    "        used_endpoints.append((endpoint_1[0], endpoint_1[1]))\n",
    "        used_endpoints.append((endpoint_2[0], endpoint_2[1]))\n",
    "\n",
    "    repeated_endpoints = set([ep for ep in used_endpoints if used_endpoints.count(ep) > 1])\n",
    "\n",
    "    pairs_no_repeated_ends: list[list[list[int, int], list[int, int], float]] = []\n",
    "    for potential_pair in potential_pairs:\n",
    "        endpoint_1, endpoint_2, distance_nm = potential_pair\n",
    "        if (endpoint_1[0], endpoint_1[1]) not in repeated_endpoints and (\n",
    "            endpoint_2[0],\n",
    "            endpoint_2[1],\n",
    "        ) not in repeated_endpoints:\n",
    "            pairs_no_repeated_ends.append(potential_pair)\n",
    "        else:\n",
    "            print(f\"excluding pair {endpoint_1}, {endpoint_2} due to repeated endpoints\")\n",
    "\n",
    "    return pairs_no_repeated_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletonisation_holearea_min_max = (0, None)\n",
    "skeletonisation_mask_smoothing_dilation_iterations = 2\n",
    "skeletonisation_mask_smoothing_gaussian_sigma = 2\n",
    "skeletonisation_method = \"topostats\"\n",
    "skeletonisation_height_bias = 0.6\n",
    "endpoint_connection_distance_nm = 10\n",
    "endpoint_connection_cost_map_height_maximum = 3.0\n",
    "\n",
    "\n",
    "for filename, file_data in loaded_files.items():\n",
    "\n",
    "    # Let's focus on this one file for now.\n",
    "    # if filename != \"20251031_nicked_picoz_8ng_nicl.0_00062\":\n",
    "    #     continue\n",
    "\n",
    "    if filename != \"20251031_nicked_picoz_8ng_nicl.0_00083\":\n",
    "        continue\n",
    "\n",
    "    print(f\"processing file: {filename}\")\n",
    "    p2nm = file_data[\"pixel_to_nm_scaling\"]\n",
    "    tensor = file_data[\"full_mask_tensor\"]\n",
    "    image = file_data[\"image\"]\n",
    "\n",
    "    channel_to_connect_ends = 1  # use the DNA channel for connecting loose ends.\n",
    "\n",
    "    mask = tensor[:, :, channel_to_connect_ends].astype(bool)\n",
    "    # plt.imshow(mask, cmap=\"gray\")\n",
    "    # plt.title(\"Original mask\")\n",
    "    # plt.show()\n",
    "\n",
    "    smoothed_mask = smooth_mask(\n",
    "        filename=filename,\n",
    "        pixel_to_nm_scaling=p2nm,\n",
    "        grain=mask,\n",
    "        gaussian_sigma=skeletonisation_mask_smoothing_gaussian_sigma,\n",
    "        holearea_min_max=skeletonisation_holearea_min_max,\n",
    "        dilation_iterations=skeletonisation_mask_smoothing_dilation_iterations,\n",
    "    )\n",
    "    # plt.imshow(smoothed_mask, cmap=\"gray\")\n",
    "    # plt.title(\"Smoothed mask\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Maybe need to check it doesn't touch the edge of the image like we do in disordered_tracing? unsure.\n",
    "\n",
    "    # Next step, skeletonize\n",
    "    skeleton = getSkeleton(\n",
    "        image=image,\n",
    "        mask=smoothed_mask,\n",
    "        method=skeletonisation_method,\n",
    "        height_bias=0.6,\n",
    "    ).get_skeleton()\n",
    "\n",
    "    # Calculate the mask width along the skeleton for later\n",
    "    mean_mask_width_nm = calculate_mask_width_with_skeleton(\n",
    "        mask=smoothed_mask,\n",
    "        skeleton=skeleton,\n",
    "        pixel_to_nm_scaling=p2nm,\n",
    "    )\n",
    "    mean_mask_width_px = mean_mask_width_nm / p2nm\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # plt.imshow(skeleton, cmap=\"gray\")\n",
    "    # plt.title(\"skeleton\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Now to find the skeleton endpoints and connect close ones.\n",
    "    convolved_skeleton = convolve_skeleton(skeleton=skeleton)\n",
    "    # Get the endpoints, value = 2\n",
    "    endpoint_coords = np.argwhere(convolved_skeleton == 2)\n",
    "    print(\"endpoints:\")\n",
    "    print(endpoint_coords)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    plt.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    grain_mask_mask = np.ma.masked_where(~mask, mask)\n",
    "    plt.imshow(grain_mask_mask, cmap=\"Blues_r\", alpha=0.3)\n",
    "    skeleton_mask = np.ma.masked_where(~convolved_skeleton.astype(bool), convolved_skeleton)\n",
    "    plt.imshow(skeleton_mask, cmap=\"viridis\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "    # For each endpoint, determine if any others are close enough to connect.\n",
    "    potential_pairs: list[list[list[int, int], list[int, int], float]] = []\n",
    "    for i, endpoint_1 in enumerate(endpoint_coords):\n",
    "        for j, endpoint_2 in enumerate(endpoint_coords):\n",
    "            if i >= j:\n",
    "                continue  # avoid double counting\n",
    "            distance_nm = np.linalg.norm((endpoint_1 - endpoint_2) * p2nm)\n",
    "            if distance_nm <= endpoint_connection_distance_nm:\n",
    "                potential_pairs.append(((endpoint_1), (endpoint_2), distance_nm))\n",
    "\n",
    "    print(\"potential pairs to connect:\")\n",
    "    for pair in potential_pairs:\n",
    "        print(pair)\n",
    "\n",
    "    # for now, for simplicity, let's delete any pairs that are involved in other pairs.\n",
    "    # Construct a list of all endpoints involved in pairs\n",
    "    potential_pairs_no_repeated_endpoints = keep_only_nonrepeated_endpoints(potential_pairs)\n",
    "    print(\"pairs to connect:\")\n",
    "    for pair in potential_pairs_no_repeated_endpoints:\n",
    "        print(pair)\n",
    "\n",
    "    for pair in potential_pairs_no_repeated_endpoints:\n",
    "        endpoint_1, endpoint_2, distance_nm = pair\n",
    "\n",
    "        # Connect the pair.\n",
    "\n",
    "        # try using height biased pathfinding between the two endpoints?\n",
    "        # create a weight cost map from the image, where 0 is the maximum cost, and the lowest cost is configurable.\n",
    "        # first create a crop around the two endpoints to speed up pathfinding\n",
    "        cost_map_bbox_padding_px = 10\n",
    "        min_y = max(0, min(endpoint_1[0], endpoint_2[0]) - cost_map_bbox_padding_px)\n",
    "        max_y = min(image.shape[0], max(endpoint_1[0], endpoint_2[0]) + cost_map_bbox_padding_px)\n",
    "        min_x = max(0, min(endpoint_1[1], endpoint_2[1]) - cost_map_bbox_padding_px)\n",
    "        max_x = min(image.shape[1], max(endpoint_1[1], endpoint_2[1]) + cost_map_bbox_padding_px)\n",
    "        cost_map = image[min_y:max_y, min_x:max_x]\n",
    "        mask_crop = mask[min_y:max_y, min_x:max_x]\n",
    "        image_crop = image[min_y:max_y, min_x:max_x]\n",
    "        local_endpoint_1 = (endpoint_1[0] - min_y, endpoint_1[1] - min_x)\n",
    "        local_endpoint_2 = (endpoint_2[0] - min_y, endpoint_2[1] - min_x)\n",
    "        # clip it to the height bounds\n",
    "        cost_map = np.clip(\n",
    "            cost_map,\n",
    "            a_min=0,\n",
    "            a_max=endpoint_connection_cost_map_height_maximum,\n",
    "        )\n",
    "        # invert it\n",
    "        cost_map = endpoint_connection_cost_map_height_maximum - cost_map\n",
    "        # normalise to 0-1\n",
    "        cost_map = cost_map / endpoint_connection_cost_map_height_maximum\n",
    "\n",
    "        # find the lowest cost path between the two endpoints\n",
    "        from skimage.graph import route_through_array\n",
    "\n",
    "        path, cost = route_through_array(\n",
    "            cost_map,\n",
    "            start=local_endpoint_1,\n",
    "            end=local_endpoint_2,\n",
    "            fully_connected=True,  # allow diagonal moves\n",
    "        )\n",
    "\n",
    "        # Dilate the path to equal the mean mask width in pixels\n",
    "        from scipy import ndimage\n",
    "        path_array = np.zeros_like(cost_map, dtype=bool)\n",
    "        # Set the path to True\n",
    "        for y, x in path:\n",
    "            path_array[y, x] = True\n",
    "        # Calculate the dilation iterations needed to reach the mean mask width\n",
    "        dilation_radius = int(np.ceil(mean_mask_width_px / 2))\n",
    "        dilated_path_array = ndimage.binary_dilation(\n",
    "            path_array,\n",
    "            iterations=dilation_radius,\n",
    "        )\n",
    "\n",
    "        # Add the dilated path to the whole mask\n",
    "        dilated_path_array_global = np.zeros_like(mask, dtype=bool)\n",
    "        dilated_path_array_global[min_y:max_y, min_x:max_x] = dilated_path_array\n",
    "        mask = mask | dilated_path_array_global\n",
    "\n",
    "        for y, x in path:\n",
    "            skeleton[y + min_y, x + min_x] = True\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "    plt.imshow(image, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    grain_mask_mask = np.ma.masked_where(~mask, mask)\n",
    "    plt.imshow(grain_mask_mask, cmap=\"Blues_r\", alpha=0.3)\n",
    "    skeleton_mask = np.ma.masked_where(~skeleton.astype(bool), skeleton)\n",
    "    plt.imshow(skeleton_mask, cmap=\"viridis\", alpha=0.7)\n",
    "    plt.title(\"skeleton after connecting loose ends\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topostats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
