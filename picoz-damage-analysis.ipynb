{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Generator, Generic\n",
    "\n",
    "from hashlib import sha256\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pydantic import BaseModel, ConfigDict, Field, computed_field\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from AFMReader.topostats import load_topostats\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from topostats.damage.damage import (\n",
    "    calculate_indirect_defect_gaps,\n",
    "    get_defects_and_gaps_from_bool_array,\n",
    ")\n",
    "from topostats.io import LoadScans\n",
    "from topostats.measure.curvature import discrete_angle_difference_per_nm_circular, total_turn_in_region_radians\n",
    "from topostats.tracing.splining import resample_points_regular_interval\n",
    "from topostats.unet_masking import make_bounding_box_square, pad_bounding_box_cutting_off_at_image_bounds\n",
    "from topostats.plottingfuncs import Colormap\n",
    "\n",
    "colormap = Colormap()\n",
    "CMAP = colormap.get_cmap()\n",
    "VMIN = -3\n",
    "VMAX = 4\n",
    "IMGPLOTARGS = {\"cmap\": CMAP, \"vmin\": VMIN, \"vmax\": VMAX}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_output():\n",
    "    from IPython.display import clear_output as ipy_clear_output\n",
    "\n",
    "    ipy_clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e2acfd",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data directories set up\n",
    "dir_base = Path(\"/Volumes/shared/pyne_group/Shared/AFM_Data/dna_damage/Cs137_irradiations\")\n",
    "assert dir_base.exists()\n",
    "dir_this_analysis = dir_base / \"20260204-analysis-getting-back-into-the-project\"\n",
    "assert dir_this_analysis.exists()\n",
    "dir_processed_data = dir_this_analysis / \"output\"\n",
    "assert dir_processed_data.exists()\n",
    "dir_results = dir_this_analysis / \"analysis_results\"\n",
    "dir_results.mkdir(exist_ok=True)\n",
    "assert dir_results.exists()\n",
    "\n",
    "# Load the data, lazily since the files are large?\n",
    "topo_files = list(dir_processed_data.glob(\"*/**/*.topostats\"))\n",
    "print(f\"found {len(topo_files)} topo files\")\n",
    "\n",
    "# Load the corresponding statistics csv file\n",
    "csv_grain_stats: Path = dir_processed_data / \"grain_statistics.csv\"\n",
    "assert csv_grain_stats.exists(), f\"could not find grain stats csv at {csv_grain_stats}\"\n",
    "df_grain_stats: pd.DataFrame = pd.read_csv(csv_grain_stats)\n",
    "print(f\"grain stats columns: {df_grain_stats.columns}\")\n",
    "\n",
    "# convert some columns to nanometres\n",
    "df_grain_stats[\"total_contour_length\"] /= 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot contour length distributions\n",
    "sns.stripplot(data=df_grain_stats, x=\"basename\", y=\"total_contour_length\", s=2)\n",
    "sns.violinplot(data=df_grain_stats, x=\"basename\", y=\"total_contour_length\", inner=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Contour length distributions\")\n",
    "plt.show()\n",
    "\n",
    "# drop any rows with contour length less than a threshold\n",
    "threshold_contour_length = 300\n",
    "\n",
    "n_rows_before = len(df_grain_stats)\n",
    "df_grain_stats = df_grain_stats[df_grain_stats[\"total_contour_length\"] >= threshold_contour_length]\n",
    "n_rows_after = len(df_grain_stats)\n",
    "print(\n",
    "    f\"dropped {n_rows_before - n_rows_after} rows with contour length < {threshold_contour_length} nm. remaining rows: {n_rows_after}\"\n",
    ")\n",
    "\n",
    "sns.stripplot(data=df_grain_stats, x=\"basename\", y=\"total_contour_length\", s=2)\n",
    "sns.violinplot(data=df_grain_stats, x=\"basename\", y=\"total_contour_length\", inner=None)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Contour length distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45cb7f",
   "metadata": {},
   "source": [
    "# Un-analysed data models (input data format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5db1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "class BaseDamageAnalysis(BaseModel):\n",
    "    \"\"\"Data object to hold settings for Models used in the project.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "\n",
    "class UnanalysedMoleculeData(BaseDamageAnalysis):\n",
    "    molecule_id: int\n",
    "    heights: npt.NDArray[np.float64]\n",
    "    distances: npt.NDArray[np.float64]\n",
    "    circular: bool\n",
    "    spline_coords: npt.NDArray[np.float64]\n",
    "    ordered_coords: npt.NDArray[np.float64]\n",
    "    curvature_data: dict | None\n",
    "\n",
    "\n",
    "class UnanalysedMoleculeDataCollection(BaseDamageAnalysis):\n",
    "    molecules: dict[int, UnanalysedMoleculeData]\n",
    "\n",
    "    def __getitem__(self, key: int) -> UnanalysedMoleculeData:\n",
    "        return self.molecules[key]\n",
    "\n",
    "    def __iter__(self) -> Generator[tuple[int, UnanalysedMoleculeData], None, None]:\n",
    "        return (item for item in self.molecules.items())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.molecules)\n",
    "\n",
    "    def __contains__(self, key: int) -> bool:\n",
    "        return key in self.molecules\n",
    "\n",
    "    def items(self) -> Generator[tuple[int, UnanalysedMoleculeData], None, None]:\n",
    "        return (item for item in self.molecules.items())\n",
    "\n",
    "    def keys(self) -> Generator[int, None, None]:\n",
    "        return (key for key in self.molecules.keys())\n",
    "\n",
    "    def values(self) -> Generator[UnanalysedMoleculeData, None, None]:\n",
    "        return (value for value in self.molecules.values())\n",
    "\n",
    "    def get(self, key: int, default: UnanalysedMoleculeData | None = None) -> UnanalysedMoleculeData | None:\n",
    "        return self.molecules.get(key, default)\n",
    "\n",
    "    def add_molecule(self, molecule: UnanalysedMoleculeData) -> None:\n",
    "        self.molecules[molecule.molecule_id] = molecule\n",
    "\n",
    "    def remove_molecule(self, molecule_id: int) -> None:\n",
    "        if molecule_id not in self.molecules:\n",
    "            raise KeyError(f\"molecule with id {molecule_id} not found in collection, cannot remove\")\n",
    "        del self.molecules[molecule_id]\n",
    "\n",
    "\n",
    "class UnanalysedGrain(BaseDamageAnalysis):\n",
    "    global_grain_id: int | None = None\n",
    "    file_grain_id: int\n",
    "    filename: str\n",
    "    pixel_to_nm_scaling: float\n",
    "    folder: str\n",
    "    percent_damage: float\n",
    "    bbox: tuple[int, int, int, int]\n",
    "    image: npt.NDArray[np.float64]\n",
    "    aspect_ratio: float\n",
    "    smallest_bounding_area: float\n",
    "    total_contour_length: float\n",
    "    num_crossings: int\n",
    "    molecule_data_collection: UnanalysedMoleculeDataCollection\n",
    "    added_left: int\n",
    "    added_top: int\n",
    "    padding: int\n",
    "    mask: npt.NDArray[np.bool_]\n",
    "    node_coords: npt.NDArray[np.float64]\n",
    "    num_nodes: int\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        # simplified string\n",
    "        return (\n",
    "            f\"GrainModel(global_grain_id={self.global_grain_id}), {self.percent_damage}% \"\n",
    "            f\"damage, from file {self.filename}.\"\n",
    "        )\n",
    "\n",
    "    def plot(self, mask_alpha: float = 0.3) -> None:\n",
    "        plt.imshow(self.image, **IMGPLOTARGS)\n",
    "        plt.imshow(self.mask[:, :, 1], alpha=mask_alpha, cmap=\"gray\")\n",
    "        plt.title(f\"grain {self.global_grain_id}, {self.percent_damage}% damage\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class UnanalysedGrainCollection(BaseDamageAnalysis):\n",
    "    grains: dict[int, UnanalysedGrain]\n",
    "    current_global_grain_id: int = 0\n",
    "\n",
    "    # pretty print\n",
    "    def __str__(self) -> str:\n",
    "        grain_indexes = range(self.current_global_grain_id)\n",
    "        missing_grain_indexes = [index for index in grain_indexes if index not in self.grains]\n",
    "        return (\n",
    "            f\"GrainModelCollection with {len(self.grains)} grains, with {len(missing_grain_indexes)} \"\n",
    "            f\"omitted grains: {missing_grain_indexes}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, key: int) -> UnanalysedGrain:\n",
    "        return self.grains[key]\n",
    "\n",
    "    def __iter__(self) -> Generator[tuple[int, UnanalysedGrain], None, None]:\n",
    "        return (item for item in self.grains.items())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.grains)\n",
    "\n",
    "    def __contains__(self, key: int) -> bool:\n",
    "        return key in self.grains\n",
    "\n",
    "    def items(self) -> Generator[tuple[int, UnanalysedGrain], None, None]:\n",
    "        return (item for item in self.grains.items())\n",
    "\n",
    "    def keys(self) -> Generator[int, None, None]:\n",
    "        return (key for key in self.grains.keys())\n",
    "\n",
    "    def values(self) -> Generator[UnanalysedGrain, None, None]:\n",
    "        return (value for value in self.grains.values())\n",
    "\n",
    "    def get(self, key: int, default: UnanalysedGrain | None = None) -> UnanalysedGrain | None:\n",
    "        return self.grains.get(key, default)\n",
    "\n",
    "    def add_grain(self, grain: UnanalysedGrain) -> None:\n",
    "        # note: a grain might already have a global grain id if it came from another collection, but we can\n",
    "        # just overwrite it.\n",
    "        grain.global_grain_id = self.current_global_grain_id\n",
    "        self.grains[self.current_global_grain_id] = grain\n",
    "        self.current_global_grain_id += 1\n",
    "\n",
    "    def remove_grain(self, global_grain_id: int) -> None:\n",
    "        if global_grain_id not in self.grains:\n",
    "            raise KeyError(f\"grain with global id {global_grain_id} not found in collection, cannot remove\")\n",
    "        del self.grains[global_grain_id]\n",
    "\n",
    "\n",
    "def combine_unanalysed_grain_collections(collections: list[UnanalysedGrainCollection]) -> UnanalysedGrainCollection:\n",
    "    combined_collection = UnanalysedGrainCollection(grains={})\n",
    "    for collection in collections:\n",
    "        for grain in collection.values():\n",
    "            combined_collection.add_grain(grain)\n",
    "    return combined_collection\n",
    "\n",
    "\n",
    "def get_dose_from_sample_type(sample_type: str) -> float:\n",
    "    if \"control\" in sample_type.lower():\n",
    "        return 0.0\n",
    "    match = re.search(r\"(\\d+)_percent_damage\", sample_type)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract dose from sample type: {sample_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31445145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files\n",
    "def load_grain_models_from_topo_files(\n",
    "    topo_files: list[Path],\n",
    "    df_grain_stats: pd.DataFrame,\n",
    "    bbox_padding: int,\n",
    "    sample_type: str,\n",
    ") -> UnanalysedGrainCollection:\n",
    "    grain_model_collection = UnanalysedGrainCollection(grains={})\n",
    "    loadscans = LoadScans(img_paths=topo_files, channel=\"dummy\")\n",
    "    loadscans.get_data()\n",
    "    loadscans_img_dict = loadscans.img_dict\n",
    "\n",
    "    # Group the dataframe by image, since we will want to load all the grains from each image at once to avoid loading\n",
    "    # the same image multiple times\n",
    "\n",
    "    unique_dir_file_combinations: set = set(zip(df_grain_stats[\"basename\"], df_grain_stats[\"image\"]))\n",
    "\n",
    "    print(f\"unique directory and file combinations: {unique_dir_file_combinations}\")\n",
    "\n",
    "    for basename, filename in unique_dir_file_combinations:\n",
    "        print(f\"extracting data for file {filename} in folder {basename}\")\n",
    "        # locate the corresponding row in the dataframe\n",
    "        df_grain_stats_image = df_grain_stats[\n",
    "            (df_grain_stats[\"image\"] == filename) & (df_grain_stats[\"basename\"] == basename)\n",
    "        ]\n",
    "        if len(df_grain_stats_image) == 0:\n",
    "            raise ValueError(\n",
    "                f\"could not find any rows in the grain stats dataframe for image {filename} and\"\n",
    "                f\"basename {basename}. this should not happen, debug this!\"\n",
    "            )\n",
    "\n",
    "        # load the corresponding image file\n",
    "        try:\n",
    "            file_data = loadscans_img_dict[filename]\n",
    "        except KeyError:\n",
    "            print(f\"keys: {list(loadscans_img_dict.keys())}\")\n",
    "            raise KeyError(f\"could not find file data for image {filename} in loaded scans. debug this!\")\n",
    "\n",
    "        full_image = file_data[\"image\"]\n",
    "        full_mask = file_data[\"grain_tensors\"][\"above\"]\n",
    "        pixel_to_nm_scaling = file_data[\"pixel_to_nm_scaling\"]\n",
    "        ordered_trace_data = file_data[\"ordered_traces\"][\"above\"]\n",
    "\n",
    "        # Grab nodestats data\n",
    "        try:\n",
    "            nodestats_data = file_data[\"nodestats\"][\"above\"][\"stats\"]\n",
    "        except KeyError:\n",
    "            nodestats_data = None\n",
    "\n",
    "        # grab individual grain data, based on each row of the dataframe\n",
    "\n",
    "        # grab the grain indexes (local) ie, 0-N for the file\n",
    "        grain_indexes_from_df = df_grain_stats_image[\"grain_number\"].unique()\n",
    "        grain_indexes_from_file = {int(grain_id.replace(\"grain_\", \"\")) for grain_id in ordered_trace_data.keys()}\n",
    "\n",
    "        if not set(grain_indexes_from_df) == grain_indexes_from_file:\n",
    "            print(\n",
    "                f\"WARN: grain indexes from dataframe and file mismatch. extra indexes in dataframe: \"\n",
    "                f\"{set(grain_indexes_from_df) - grain_indexes_from_file} extra indexes in \"\n",
    "                f\"file: {grain_indexes_from_file - set(grain_indexes_from_df)}\"\n",
    "            )\n",
    "\n",
    "        # Now we can confidently grab the grain data based on the dataframe\n",
    "        for grain_index in grain_indexes_from_df:\n",
    "            grain_id_str = f\"grain_{grain_index}\"\n",
    "            if grain_id_str not in ordered_trace_data:\n",
    "                print(\n",
    "                    f\"WARN: grain id {grain_id_str} from dataframe not found in file data. THIS SHOULD NOT HAPPEN, DEBUG THIS!\"\n",
    "                )\n",
    "                continue\n",
    "            grain_ordered_trace_data = ordered_trace_data[grain_id_str]\n",
    "            df_grain_stats_grain = df_grain_stats_image[df_grain_stats_image[\"grain_number\"] == grain_index]\n",
    "            assert (\n",
    "                len(df_grain_stats_grain) == 1\n",
    "            ), f\"expected exactly one row in the grain stats dataframe for grain {grain_index} in\"\n",
    "            f\"image {filename}, but found {len(df_grain_stats_grain)}. DEBUG THIS!\"\n",
    "\n",
    "            dose_percentage = get_dose_from_sample_type(sample_type)\n",
    "            aspect_ratio = df_grain_stats_grain[\"aspect_ratio\"].values[0]\n",
    "            total_contour_length = df_grain_stats_grain[\"total_contour_length\"].values[0]\n",
    "            num_crossings = df_grain_stats_grain[\"num_crossings\"].values[0]\n",
    "            smallest_bounding_area = df_grain_stats_grain[\"smallest_bounding_area\"].values[0]\n",
    "\n",
    "            # get the molecule data\n",
    "            molecule_data_collection = UnanalysedMoleculeDataCollection(molecules={})\n",
    "            for current_molecule_id_str, molecule_ordered_trace_data in grain_ordered_trace_data.items():\n",
    "                print(f\"-- processing molecule {current_molecule_id_str}\")\n",
    "                molecule_id = int(re.sub(r\"mol_\", \"\", current_molecule_id_str))\n",
    "                ordered_coords = molecule_ordered_trace_data[\"ordered_coords\"]\n",
    "                molecule_data_heights = molecule_ordered_trace_data[\"heights\"]\n",
    "                molecule_data_distances = molecule_ordered_trace_data[\"distances\"]\n",
    "                molecule_data_circular = molecule_ordered_trace_data[\"mol_stats\"][\"circular\"]\n",
    "                bbox = molecule_ordered_trace_data[\"bbox\"]\n",
    "\n",
    "                splining_coords = file_data[\"splining\"][\"above\"][grain_id_str][current_molecule_id_str][\"spline_coords\"]\n",
    "                try:\n",
    "                    curvature_data_grains = file_data[\"grain_curvature_stats\"][\"above\"][\"grains\"]\n",
    "                    curvature_data_grain = curvature_data_grains[grain_id_str]\n",
    "                    curvature_data_molecules = curvature_data_grain[\"molecules\"]\n",
    "                    curvature_data_molecule = curvature_data_molecules[current_molecule_id_str]\n",
    "                    molecule_data_curvature_data = curvature_data_molecule\n",
    "                except KeyError:\n",
    "                    print(\n",
    "                        f\"could not find curvature data for grain {grain_id_str}\"\n",
    "                        f\"molecule {current_molecule_id_str}), setting to None. file keys: {list(file_data.keys())}\"\n",
    "                    )\n",
    "                    molecule_data_curvature_data = None\n",
    "\n",
    "                # bbox will be the same for all molecules so this is okay\n",
    "                bbox_square = make_bounding_box_square(\n",
    "                    crop_min_row=bbox[0],\n",
    "                    crop_min_col=bbox[1],\n",
    "                    crop_max_row=bbox[2],\n",
    "                    crop_max_col=bbox[3],\n",
    "                    image_shape=full_image.shape,\n",
    "                )\n",
    "                bbox_padded = pad_bounding_box_cutting_off_at_image_bounds(\n",
    "                    crop_min_row=bbox_square[0],\n",
    "                    crop_min_col=bbox_square[1],\n",
    "                    crop_max_row=bbox_square[2],\n",
    "                    crop_max_col=bbox_square[3],\n",
    "                    image_shape=full_image.shape,\n",
    "                    padding=bbox_padding,\n",
    "                )\n",
    "                added_left = bbox_padded[1] - bbox[1]\n",
    "                added_top = bbox_padded[0] - bbox[0]\n",
    "\n",
    "                # adjust the splining coords to account for padding\n",
    "                splining_coords[:, 0] -= added_top\n",
    "                splining_coords[:, 1] -= added_left\n",
    "                molecule_data_spline_coords = splining_coords\n",
    "\n",
    "                # adjust the ordered coords to account for padding\n",
    "                ordered_coords[:, 0] -= added_top\n",
    "                ordered_coords[:, 1] -= added_left\n",
    "                molecule_data_ordered_coords = ordered_coords\n",
    "\n",
    "                molecule_data = UnanalysedMoleculeData(\n",
    "                    molecule_id=molecule_id,\n",
    "                    heights=molecule_data_heights,\n",
    "                    distances=molecule_data_distances,\n",
    "                    circular=molecule_data_circular,\n",
    "                    curvature_data=molecule_data_curvature_data,\n",
    "                    spline_coords=molecule_data_spline_coords,\n",
    "                    ordered_coords=molecule_data_ordered_coords,\n",
    "                )\n",
    "                molecule_data_collection.add_molecule(molecule_data)\n",
    "\n",
    "            mask = full_mask[\n",
    "                bbox_padded[0] : bbox_padded[2],\n",
    "                bbox_padded[1] : bbox_padded[3],\n",
    "            ]\n",
    "\n",
    "            image = full_image[\n",
    "                bbox_padded[0] : bbox_padded[2],\n",
    "                bbox_padded[1] : bbox_padded[3],\n",
    "            ]\n",
    "\n",
    "            all_node_coords = []\n",
    "            num_nodes: int = 0\n",
    "            if nodestats_data is not None:\n",
    "                try:\n",
    "                    grain_nodestats_data = nodestats_data[grain_id_str]\n",
    "                    num_nodes = len(grain_nodestats_data)\n",
    "                    for _node_index, node_data in grain_nodestats_data.items():\n",
    "                        node_coords = node_data[\"coords\"]\n",
    "                        # adjust the node coords to account for padding\n",
    "                        node_coords[:, 0] -= added_top\n",
    "                        node_coords[:, 1] -= added_left\n",
    "                        for node_coord in node_coords:\n",
    "                            all_node_coords.append(node_coord)\n",
    "                except KeyError as e:\n",
    "                    if \"grain_\" in str(e):\n",
    "                        # grain has no nodestats data here, skip\n",
    "                        pass\n",
    "            all_node_coords_array = np.array(all_node_coords)\n",
    "\n",
    "            grain_model = UnanalysedGrain(\n",
    "                file_grain_id=grain_index,\n",
    "                filename=filename,\n",
    "                pixel_to_nm_scaling=pixel_to_nm_scaling,\n",
    "                folder=str(sample_type),\n",
    "                percent_damage=dose_percentage,\n",
    "                bbox=bbox_padded,\n",
    "                image=image,\n",
    "                mask=mask,\n",
    "                aspect_ratio=aspect_ratio,\n",
    "                total_contour_length=total_contour_length,\n",
    "                num_crossings=num_crossings,\n",
    "                molecule_data_collection=molecule_data_collection,\n",
    "                added_left=added_left,\n",
    "                added_top=added_top,\n",
    "                padding=bbox_padding,\n",
    "                node_coords=all_node_coords_array,\n",
    "                num_nodes=num_nodes,\n",
    "                smallest_bounding_area=smallest_bounding_area,\n",
    "            )\n",
    "\n",
    "            grain_model_collection.add_grain(grain_model)\n",
    "    return grain_model_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fbed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the grains from the topostats files\n",
    "\n",
    "\n",
    "def construct_grains_collection_from_topostats_files(\n",
    "    bbox_padding: int, force_reload: bool = False\n",
    ") -> UnanalysedGrainCollection:\n",
    "    grain_model_collection = UnanalysedGrainCollection(grains={})\n",
    "    dir_loaded_datasets = dir_results / \"loaded_datasets\"\n",
    "    assert dir_loaded_datasets.exists(), f\"could not find dataset hash directory at {dir_loaded_datasets}\"\n",
    "\n",
    "    # iterate through each topostats image file that remains in the grain stats dataframe and extract the grain data\n",
    "    # from the topostats files and store them.\n",
    "    unique_file_folder_combinations = df_grain_stats[[\"image\", \"basename\"]].drop_duplicates()\n",
    "    print(f\"found {len(unique_file_folder_combinations)} unique topostats files in the grain stats dataframe\")\n",
    "\n",
    "    # Split the files by folder\n",
    "    unique_file_folder_combinations_grouped = unique_file_folder_combinations.groupby(\"basename\")\n",
    "    print(unique_file_folder_combinations_grouped.groups.keys())\n",
    "\n",
    "    # calculate a hash for each folder by combining the hashes of files in that folder.\n",
    "    for local_folder, group in unique_file_folder_combinations_grouped:\n",
    "        sample_type = local_folder.replace(\"../all_data/\", \"\").replace(\"../test_subset_data/\", \"\")\n",
    "        dir_loaded_sample_type = dir_loaded_datasets / sample_type\n",
    "        print(f\"checking folder {sample_type} with {len(group)} files\")\n",
    "        print(f\"calculating hashes for topostats files...\")\n",
    "        file_paths_and_hashes_topostats: dict[Path, str] = {}\n",
    "        # construct the file paths for each unique combination of image and folder and check if it exists.\n",
    "        for _, row in group.iterrows():\n",
    "            filename = row[\"image\"]\n",
    "            basename = row[\"basename\"]\n",
    "            # reconstruct the path to the file using the basename, image name and structure of directories.\n",
    "            if \"all_data\" in basename:\n",
    "                dir_topo_file = Path(str(basename).replace(\"../all_data/\", \"\"))\n",
    "            if \"test_subset_data\" in basename:\n",
    "                dir_topo_file = Path(str(basename).replace(\"../test_subset_data/\", \"\"))\n",
    "            dir_topo_file = dir_processed_data / dir_topo_file / \"processed\"\n",
    "            assert dir_topo_file.exists(), f\"could not find folder at {dir_topo_file}\"\n",
    "            file_topostats = dir_topo_file / f\"{filename}.topostats\"\n",
    "            assert file_topostats.exists(), f\"could not find topostats file at {file_topostats}\"\n",
    "\n",
    "            # calculate the hash of the file\n",
    "            file_topostats_hash = sha256(file_topostats.read_bytes()).hexdigest()\n",
    "            file_paths_and_hashes_topostats[file_topostats] = file_topostats_hash\n",
    "\n",
    "        # create hash for the folder by combining the hashes of the files in the folder\n",
    "        combined_hash_string = \"\".join(sorted(file_paths_and_hashes_topostats.values()))\n",
    "        folder_hash = sha256(combined_hash_string.encode()).hexdigest()\n",
    "\n",
    "        # load the previous hash from text file if it exists\n",
    "        previous_hash_file = dir_loaded_sample_type / \"hash.txt\"\n",
    "        print(f\"checking previous hash for folder {sample_type} at {previous_hash_file}\")\n",
    "        previous_hash = None\n",
    "        if previous_hash_file.exists():\n",
    "            previous_hash = previous_hash_file.read_text()\n",
    "\n",
    "        print(\n",
    "            f\"folder {sample_type} hash calculated: {folder_hash}. previous hash: {previous_hash} force reload: {force_reload}\"\n",
    "        )\n",
    "\n",
    "        if previous_hash == folder_hash and not force_reload:\n",
    "            # check if the saved data for this folder exists and if it does, load it instead of loading the data\n",
    "            # from the topostats files\n",
    "            file_previous_loaded_data = dir_loaded_sample_type / f\"data.pkl\"\n",
    "            if file_previous_loaded_data.exists():\n",
    "                print(\n",
    "                    f\"folder {sample_type} has not changed since last load, skipping loading it and using previous saved data\"\n",
    "                )\n",
    "                with open(file_previous_loaded_data, \"rb\") as f:\n",
    "                    grain_model_collection_folder: UnanalysedGrainCollection = pickle.load(f)\n",
    "                    # Combine the grain model collection for this folder with the main grain model collection\n",
    "                    grain_model_collection = combine_unanalysed_grain_collections(\n",
    "                        [grain_model_collection, grain_model_collection_folder]\n",
    "                    )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"hash for folder {sample_type} matched previous, but could not locate \"\n",
    "                    f\"saved data: {file_previous_loaded_data}\"\n",
    "                )\n",
    "        else:\n",
    "            if not force_reload:\n",
    "                print(\n",
    "                    f\"folder {sample_type} hash has changed since last load, loading the data from the topostats files\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"forcing reload, loading the data from the topostats files for folder {sample_type}\")\n",
    "\n",
    "            # calculate a subset of the dataframe for just this folder\n",
    "            df_grain_stats_folder = df_grain_stats[df_grain_stats[\"basename\"] == local_folder]\n",
    "            grain_model_collection_folder = load_grain_models_from_topo_files(\n",
    "                topo_files=list(file_paths_and_hashes_topostats.keys()),\n",
    "                df_grain_stats=df_grain_stats_folder,\n",
    "                bbox_padding=bbox_padding,\n",
    "                sample_type=sample_type,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"loaded topostats file data into grain model collection.\"\n",
    "                \"Saving loaded data to .pkl file and saving hash.\"\n",
    "            )\n",
    "\n",
    "            # after loading all the grains for the folder, save the model to a pickle and save the hash\n",
    "            # for the folder\n",
    "            file_to_save = dir_loaded_sample_type / f\"data.pkl\"\n",
    "            # ensure the parent folder is created\n",
    "            file_to_save.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(file_to_save, \"wb\") as f:\n",
    "                pickle.dump(grain_model_collection_folder, f)\n",
    "            # save the hash for the folder\n",
    "            print(f\"saving hash for folder {sample_type} to {previous_hash_file}\")\n",
    "            previous_hash_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(previous_hash_file, \"w\") as f:\n",
    "                f.write(folder_hash)\n",
    "\n",
    "            # combine the grain model collection for this folder with the main grain model collection\n",
    "            grain_model_collection = combine_unanalysed_grain_collections(\n",
    "                [grain_model_collection, grain_model_collection_folder]\n",
    "            )\n",
    "\n",
    "    return grain_model_collection\n",
    "\n",
    "\n",
    "grain_collection = construct_grains_collection_from_topostats_files(bbox_padding=1, force_reload=True)\n",
    "print(grain_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6142e",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411ffc9",
   "metadata": {},
   "source": [
    "## Analysed data models (analysed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defect(BaseDamageAnalysis):\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    length_nm: float\n",
    "    position_along_trace_nm: float\n",
    "    total_turn_radians: tuple[float, float]\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if not isinstance(other, Defect):\n",
    "            raise TypeError(f\"Cannot compare Defect with {type(other)}\")\n",
    "        return (\n",
    "            self.start_index == other.start_index\n",
    "            and self.end_index == other.end_index\n",
    "            and np.isclose(self.length_nm, other.length_nm, rtol=1e-9, atol=1e-12)\n",
    "            and np.isclose(self.position_along_trace_nm, other.position_along_trace_nm, rtol=1e-9, atol=1e-12)\n",
    "            and np.isclose(self.total_turn_radians[0], other.total_turn_radians[0], rtol=1e-9, atol=1e-12)\n",
    "            and np.isclose(self.total_turn_radians[1], other.total_turn_radians[1], rtol=1e-9, atol=1e-12)\n",
    "        )\n",
    "\n",
    "\n",
    "class DefectGap(BaseDamageAnalysis):\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    length_nm: float\n",
    "    position_along_trace_nm: float\n",
    "\n",
    "\n",
    "class OrderedDefectsGaps(BaseDamageAnalysis):\n",
    "    defect_gap_list: list[Defect | DefectGap] = Field(default_factory=list)\n",
    "\n",
    "    # post-init to sort the list by start index\n",
    "    def model_post_init(self, __context: dict | None = None) -> None:\n",
    "        self.sort_defect_gap_list()\n",
    "\n",
    "    def sort_defect_gap_list(self) -> None:\n",
    "        self.defect_gap_list.sort(key=lambda x: x.start_index)\n",
    "\n",
    "    def add_item(self, item: Defect | DefectGap) -> None:\n",
    "        self.defect_gap_list.append(item)\n",
    "        self.sort_defect_gap_list()\n",
    "\n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        if not isinstance(other, OrderedDefectsGaps):\n",
    "            raise TypeError(f\"Cannot compare OrderedDefectsGaps with {type(other)}\")\n",
    "        if len(self.defect_gap_list) != len(other.defect_gap_list):\n",
    "            return False\n",
    "\n",
    "        for item_self, item_other in zip(self.defect_gap_list, other.defect_gap_list):\n",
    "            if item_self != item_other:\n",
    "                return False\n",
    "\n",
    "\n",
    "class DefectData(BaseDamageAnalysis):\n",
    "    ordered_defects_and_gaps: OrderedDefectsGaps\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def num_defects(self) -> int:\n",
    "        return sum(isinstance(item, Defect) for item in self.ordered_defects_and_gaps.defect_gap_list)\n",
    "\n",
    "    @computed_field\n",
    "    @property\n",
    "    def num_gaps(self) -> int:\n",
    "        return sum(isinstance(item, DefectGap) for item in self.ordered_defects_and_gaps.defect_gap_list)\n",
    "\n",
    "\n",
    "class MoleculeData(UnanalysedMoleculeData):\n",
    "\n",
    "    def from_unanalysed_molecule_data(unanalysed_data: UnanalysedMoleculeData) -> \"MoleculeData\":\n",
    "        return MoleculeData(\n",
    "            molecule_id=unanalysed_data.molecule_id,\n",
    "            heights=unanalysed_data.heights,\n",
    "            distances=unanalysed_data.distances,\n",
    "            circular=unanalysed_data.circular,\n",
    "            spline_coords=unanalysed_data.spline_coords,\n",
    "            ordered_coords=unanalysed_data.ordered_coords,\n",
    "            curvature_data=unanalysed_data.curvature_data,\n",
    "        )\n",
    "\n",
    "\n",
    "class MoleculeDataCollection(UnanalysedMoleculeDataCollection):\n",
    "\n",
    "    def from_unanalysed_molecule_data_collection(\n",
    "        unanalysed_collection: UnanalysedMoleculeDataCollection,\n",
    "    ) -> \"MoleculeDataCollection\":\n",
    "        molecule_data_dict = {}\n",
    "        for molecule_id, unanalysed_molecule_data in unanalysed_collection.molecules.items():\n",
    "            molecule_data = MoleculeData.from_unanalysed_molecule_data(unanalysed_molecule_data)\n",
    "            molecule_data_dict[molecule_id] = molecule_data\n",
    "        return MoleculeDataCollection(molecules=molecule_data_dict)\n",
    "\n",
    "\n",
    "class GrainModel(UnanalysedGrain):\n",
    "\n",
    "    defect_data: DefectData | None = None\n",
    "\n",
    "    def from_unanalysed_grain(unanalysed_grain: UnanalysedGrain) -> \"GrainModel\":\n",
    "        # Create the new molecule data collection\n",
    "        molecule_data_collection = MoleculeDataCollection.from_unanalysed_molecule_data_collection(\n",
    "            unanalysed_grain.molecule_data_collection\n",
    "        )\n",
    "        return GrainModel(\n",
    "            global_grain_id=unanalysed_grain.global_grain_id,\n",
    "            file_grain_id=unanalysed_grain.file_grain_id,\n",
    "            filename=unanalysed_grain.filename,\n",
    "            pixel_to_nm_scaling=unanalysed_grain.pixel_to_nm_scaling,\n",
    "            folder=unanalysed_grain.folder,\n",
    "            percent_damage=unanalysed_grain.percent_damage,\n",
    "            bbox=unanalysed_grain.bbox,\n",
    "            image=unanalysed_grain.image,\n",
    "            aspect_ratio=unanalysed_grain.aspect_ratio,\n",
    "            smallest_bounding_area=unanalysed_grain.smallest_bounding_area,\n",
    "            total_contour_length=unanalysed_grain.total_contour_length,\n",
    "            num_crossings=unanalysed_grain.num_crossings,\n",
    "            molecule_data_collection=molecule_data_collection,\n",
    "            added_left=unanalysed_grain.added_left,\n",
    "            added_top=unanalysed_grain.added_top,\n",
    "            padding=unanalysed_grain.padding,\n",
    "            mask=unanalysed_grain.mask,\n",
    "            node_coords=unanalysed_grain.node_coords,\n",
    "            num_nodes=unanalysed_grain.num_nodes,\n",
    "        )\n",
    "\n",
    "    def plot(self, mask_alpha: float = 0.3, linemode: str = \"\") -> None:\n",
    "        plt.imshow(self.image, **IMGPLOTARGS)\n",
    "        plt.imshow(self.mask[:, :, 1], alpha=mask_alpha, cmap=\"gray\")\n",
    "        if linemode == \"spline\":\n",
    "            for molecule_id, molecule_data in self.molecule_data_collection.items():\n",
    "                spline_coords = molecule_data.spline_coords\n",
    "                plt.plot(spline_coords[:, 1], spline_coords[:, 0])\n",
    "        elif linemode == \"curvature\":\n",
    "            for molecule_id, molecule_data in self.molecule_data_collection.items():\n",
    "                spline_coords = molecule_data.spline_coords\n",
    "                curvature_data = molecule_data.curvature_data\n",
    "                if curvature_data is not None:\n",
    "                    curvature_values = curvature_data[\"curvatures\"]\n",
    "                    # plot the curvature values as a colormap along the spline coords\n",
    "                    assert len(curvature_values) == len(spline_coords), (\n",
    "                        f\"length of curvature values {len(curvature_values)} does not match\"\n",
    "                        f\"length of spline coords {len(spline_coords)}\"\n",
    "                    )\n",
    "                    curvature_norm_bounds_lower = -0.1\n",
    "                    curvature_norm_bounds_upper = 0.1\n",
    "                    curvature_values_clipped = np.clip(\n",
    "                        curvature_values, curvature_norm_bounds_lower, curvature_norm_bounds_upper\n",
    "                    )\n",
    "                    curvature_values_normalised = (curvature_values_clipped - curvature_norm_bounds_lower) / (\n",
    "                        curvature_norm_bounds_upper - curvature_norm_bounds_lower\n",
    "                    )\n",
    "                    curvature_cmap = mpl.cm.coolwarm\n",
    "                    for index, point in enumerate(spline_coords):\n",
    "                        color = curvature_cmap(curvature_values_normalised[index])\n",
    "                        if index > 0:\n",
    "                            previous_point = spline_coords[index - 1]\n",
    "                            plt.plot(\n",
    "                                [previous_point[1], point[1]],\n",
    "                                [previous_point[0], point[0]],\n",
    "                                color=color,\n",
    "                                linewidth=1,\n",
    "                            )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class GrainCollection(UnanalysedGrainCollection):\n",
    "\n",
    "    def from_unanalysed_grain_collection(\n",
    "        unanalysed_collection: UnanalysedGrainCollection,\n",
    "    ) -> \"GrainCollection\":\n",
    "        grain_dict = {}\n",
    "        for global_grain_id, unanalysed_grain in unanalysed_collection.grains.items():\n",
    "            grain_model = GrainModel.from_unanalysed_grain(unanalysed_grain)\n",
    "            grain_dict[global_grain_id] = grain_model\n",
    "        return GrainCollection(grains=grain_dict, current_global_grain_id=unanalysed_collection.current_global_grain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create current analysis models from the loaded models, which allow us to add more things in this notebook without\n",
    "# having to re-load the data each time to initialise fresh versions of the unanalysed models.\n",
    "\n",
    "grain_collection = GrainCollection.from_unanalysed_grain_collection(grain_collection)\n",
    "print(grain_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d241b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defect detection\n",
    "first_grain = next(iter(grain_collection.values()))\n",
    "print(first_grain)\n",
    "\n",
    "\n",
    "def find_curvature_defects(\n",
    "    grain_model_collection: GrainCollection,\n",
    "    curvature_defect_method: str,\n",
    "    curvature_threshold_iqr_multiplier: float,\n",
    "    curvature_threshold_absolute_pernm: float,\n",
    ") -> set[int]:\n",
    "    # find curvature defects\n",
    "    bad_grains = set()\n",
    "    if curvature_defect_method == \"iqr\":\n",
    "        # iterate over each grain\n",
    "        for global_grain_id, grain_model in grain_model_collection.items():\n",
    "            for molecule_id, molecule_data in grain_model.molecule_data_collection.items():\n",
    "                molecule_data_curvature_data = molecule_data.curvature_data\n",
    "                if molecule_data_curvature_data is None:\n",
    "                    print(\n",
    "                        f\"no curvature data for grain {global_grain_id} molecule {molecule_id}, skipping curvature defect detection for this molecule\"\n",
    "                    )\n",
    "                    bad_grains.add(global_grain_id)\n",
    "                    continue\n",
    "                curvatures = molecule_data_curvature_data[\"curvatures\"]\n",
    "                assert isinstance(\n",
    "                    curvatures, np.ndarray\n",
    "                ), f\"expected curvatures to be a numpy array, but got {type(curvatures)}\"\n",
    "                pass\n",
    "\n",
    "    return bad_grains\n",
    "\n",
    "\n",
    "def find_defects_in_height_and_curvature(\n",
    "    grain_model_collection: GrainCollection,\n",
    "    height_defect_method: str,\n",
    "    height_threshold_iqr_multiplier: float,\n",
    "    height_threshold_absolute_nm: float,\n",
    "    curvature_defect_method: str,\n",
    "    curvature_threshold_iqr_multiplier: float,\n",
    "    curvature_threshold_absolute_pernm: float,\n",
    ") -> set[int]:\n",
    "\n",
    "    bad_grains = set()\n",
    "\n",
    "    # find curvature defects\n",
    "    additional_bad_grains = find_curvature_defects(\n",
    "        grain_model_collection=grain_model_collection,\n",
    "        curvature_defect_method=curvature_defect_method,\n",
    "        curvature_threshold_iqr_multiplier=curvature_threshold_iqr_multiplier,\n",
    "        curvature_threshold_absolute_pernm=curvature_threshold_absolute_pernm,\n",
    "    )\n",
    "    bad_grains.update(additional_bad_grains)\n",
    "\n",
    "    return bad_grains\n",
    "\n",
    "\n",
    "bad_grains = find_defects_in_height_and_curvature(\n",
    "    grain_model_collection=grain_collection,\n",
    "    height_defect_method=\"iqr\",\n",
    "    height_threshold_iqr_multiplier=1.5,\n",
    "    height_threshold_absolute_nm=0.8,\n",
    "    curvature_defect_method=\"iqr\",\n",
    "    curvature_threshold_iqr_multiplier=1.5,\n",
    "    curvature_threshold_absolute_pernm=0.1,\n",
    ")\n",
    "\n",
    "# remove bad grains\n",
    "for bad_grain_id in bad_grains:\n",
    "    grain_collection.remove_grain(bad_grain_id)\n",
    "\n",
    "print(grain_collection)\n",
    "\n",
    "sample_grain = next(iter(grain_collection.values()))\n",
    "sample_grain.plot(mask_alpha=0.1, linemode=\"curvature\")\n",
    "print(sample_grain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topostats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
