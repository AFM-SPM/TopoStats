{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union, Dict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from skimage.morphology import binary_dilation, skeletonize\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.color import label2rgb\n",
    "from skimage.graph import route_through_array\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import seaborn as sns\n",
    "from skimage.morphology import binary_erosion\n",
    "from scipy.ndimage import binary_fill_holes\n",
    "from skimage.feature import canny\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "from topostats.grain_finding_haribo_unet import (\n",
    "    predict_unet,\n",
    "    load_model,\n",
    "    predict_unet_multiclass_and_get_angle,\n",
    "    mean_iou,\n",
    "    iou,\n",
    "    predict_unet_multiclass,\n",
    ")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from topostats.plottingfuncs import Colormap\n",
    "\n",
    "colormap = Colormap()\n",
    "CMAP = colormap.get_cmap()\n",
    "\n",
    "VMIN = 0\n",
    "VMAX = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get grain crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = \"OT2_SC\"\n",
    "MAX_P_TO_NM = 10.0\n",
    "PLOT_RESULTS = True\n",
    "SIMPLE_HEIGHT_THRESHOLD = 1.0\n",
    "SIMPLE_CROP_PADDING = 5\n",
    "SIMPLE_AREA_THRESHOLDS = (500, 100000000000)\n",
    "FLATTENED_IMAGE_DIR = Path(\n",
    "    \"/Volumes/shared/pyne_group/Shared/AFM_Data/Cas9_Minicircles/Analysis_all/DNA_Cas9/output_justboundcas9\"\n",
    ")\n",
    "assert FLATTENED_IMAGE_DIR.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data files\n",
    "sample_data_dir = FLATTENED_IMAGE_DIR / SAMPLE / \"processed\"\n",
    "data_files = list(sample_data_dir.glob(\"*.topostats\"))\n",
    "print(f\"num files: {len(data_files)}\")\n",
    "\n",
    "# Load the image from the hdf5 files\n",
    "flattened_image_dict = {}\n",
    "\n",
    "for image_index, data_file in enumerate(data_files):\n",
    "    with h5py.File(data_file, \"r\") as f:\n",
    "        # print(f\"keys: {list(f.keys())}\")\n",
    "        flattened_image = f[\"image\"][:]\n",
    "        p_to_nm = f[\"pixel_to_nm_scaling\"][()]\n",
    "        if p_to_nm > MAX_P_TO_NM:\n",
    "            # print(f\"Skipping image {data_file} due to large p_to_nm: {p_to_nm}\")\n",
    "            continue\n",
    "        flattened_image_dict[image_index] = {\n",
    "            \"image\": flattened_image,\n",
    "            \"p_to_nm\": p_to_nm,\n",
    "            \"file_path\": data_file,\n",
    "        }\n",
    "    print(f\"Loaded image {data_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each image, threshold generously and crop the regions\n",
    "\n",
    "grain_crop_dictionaries = {}\n",
    "masked_image_dictionaries = {}\n",
    "for image_index, image_dict in flattened_image_dict.items():\n",
    "    image = image_dict[\"image\"]\n",
    "    p_to_nm = image_dict[\"p_to_nm\"]\n",
    "    file_path = image_dict[\"file_path\"]\n",
    "\n",
    "    # Threshold the image\n",
    "    simple_mask = image > SIMPLE_HEIGHT_THRESHOLD\n",
    "\n",
    "    # plt.imshow(image, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "    # plt.show()\n",
    "    # plt.imshow(simple_mask)\n",
    "    # plt.show()\n",
    "\n",
    "    # Label\n",
    "    labelled_simple_mask = label(simple_mask)\n",
    "\n",
    "    # Vet the regions based on size\n",
    "    region_props = regionprops(labelled_simple_mask)\n",
    "\n",
    "    vetted_bounding_boxes = []\n",
    "\n",
    "    for region in region_props:\n",
    "        area_nm = region.area * p_to_nm**2\n",
    "        if region.area < SIMPLE_AREA_THRESHOLDS[0] or region.area > SIMPLE_AREA_THRESHOLDS[1]:\n",
    "            # print(f\"Skipping region with area {area_nm} nm^2\")\n",
    "            # remove the region from the labeled mask\n",
    "            labelled_simple_mask[labelled_simple_mask == region.label] = 0\n",
    "            continue\n",
    "\n",
    "        # Get the bounding box\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "\n",
    "        # Add padding to the crop\n",
    "        minr = minr - SIMPLE_CROP_PADDING\n",
    "        minc = minc - SIMPLE_CROP_PADDING\n",
    "        maxr = maxr + SIMPLE_CROP_PADDING\n",
    "        maxc = maxc + SIMPLE_CROP_PADDING\n",
    "\n",
    "        maxr = max(maxr, maxc - minc + minr)\n",
    "        maxc = max(maxc, maxr - minr + minc)\n",
    "\n",
    "        # Check if the crop is out of bounds\n",
    "        if minr < 0 or minc < 0 or maxr > image.shape[0] or maxc > image.shape[1]:\n",
    "            # print(f\"Skipping region with area {area_nm} nm^2 due to out of bounds crop\")\n",
    "            # remove the region from the labeled mask\n",
    "            labelled_simple_mask[labelled_simple_mask == region.label] = 0\n",
    "            continue\n",
    "\n",
    "        # Crop the image\n",
    "        cropped_image = image[minr:maxr, minc:maxc]\n",
    "        cropped_mask = simple_mask[minr:maxr, minc:maxc]\n",
    "\n",
    "        # Save the cropped image\n",
    "        grain_crop_dictionaries[(image_index)] = {\n",
    "            \"cropped_image\": cropped_image,\n",
    "            \"cropped_mask\": cropped_mask,\n",
    "            \"region\": region,\n",
    "            \"p_to_nm\": p_to_nm,\n",
    "            \"file_path\": file_path,\n",
    "            \"minr\": minr,\n",
    "            \"minc\": minc,\n",
    "            \"maxr\": maxr,\n",
    "            \"maxc\": maxc,\n",
    "        }\n",
    "\n",
    "        vetted_bounding_boxes.append((minr, minc, maxr, maxc))\n",
    "\n",
    "    # save the mask with vetted regions\n",
    "    masked_image_dictionaries[image_index] = {\n",
    "        \"image\": image,\n",
    "        \"p_to_nm\": p_to_nm,\n",
    "        \"file_path\": file_path,\n",
    "        \"simple_mask\": simple_mask,\n",
    "        \"vetted_bounding_boxes\": vetted_bounding_boxes,\n",
    "    }\n",
    "\n",
    "    # plt.imshow(labelled_simple_mask)\n",
    "\n",
    "print(f\"found {len(grain_crop_dictionaries)} cropped grains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random selection of 10 images\n",
    "\n",
    "max_index = len(flattened_image_dict)\n",
    "indices = np.random.choice(max_index, 10, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(10, 2, figsize=(10, 5 * len(indices)))\n",
    "# Plot images, simple masks and bounding boxes\n",
    "for i, index in enumerate(indices):\n",
    "    image = masked_image_dictionaries[index][\"image\"]\n",
    "    simple_mask = masked_image_dictionaries[index][\"simple_mask\"]\n",
    "    vetted_bounding_boxes = masked_image_dictionaries[index][\"vetted_bounding_boxes\"]\n",
    "\n",
    "    axes[i, 0].imshow(image, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "    axes[i, 0].set_title(f\"Image {index}\")\n",
    "\n",
    "    axes[i, 1].imshow(simple_mask)\n",
    "    axes[i, 1].set_title(f\"Simple mask {index}\")\n",
    "\n",
    "    for minr, minc, maxr, maxc in vetted_bounding_boxes:\n",
    "        rect = plt.Rectangle((minc, minr), maxc - minc, maxr - minr, fill=False, edgecolor=\"red\")\n",
    "        axes[i, 0].add_patch(rect)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a gallery of all the cropped grains\n",
    "\n",
    "\n",
    "def plot_images(\n",
    "    images: list,\n",
    "    masks: list,\n",
    "    grain_indexes: list,\n",
    "    px_to_nms: list,\n",
    "    file_paths: list,\n",
    "    width=3,\n",
    "    VMIN=VMIN,\n",
    "    VMAX=VMAX,\n",
    "    cmap=CMAP,\n",
    "):\n",
    "    num_images = len(images)\n",
    "    num_rows = num_images // width + 1\n",
    "    num_images_in_batch = 2\n",
    "    fig, axes = plt.subplots(num_rows, width * num_images_in_batch, figsize=(width * 20, num_rows * 8))\n",
    "    for i, (image, mask, grain_index, p_to_nm, file_path) in enumerate(\n",
    "        zip(images, masks, grain_indexes, px_to_nms, file_paths)\n",
    "    ):\n",
    "        # Plot image\n",
    "        im_ax = axes[i // width, i % width * num_images_in_batch]\n",
    "        im_ax.imshow(image, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "        im_ax.set_title(f\"Grain {grain_index} {p_to_nm:.2f} p/nm \\n {file_path.stem}\", fontsize=20)\n",
    "        im_ax.axis(\"off\")\n",
    "        # Plot mask\n",
    "        mask_ax = axes[i // width, i % width * num_images_in_batch + 1]\n",
    "        mask_ax.imshow(mask.astype(int))\n",
    "        mask_ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the gallery\n",
    "plot_images(\n",
    "    images=[grain_crop_dictionaries[i][\"cropped_image\"] for i in grain_crop_dictionaries],\n",
    "    masks=[grain_crop_dictionaries[i][\"cropped_mask\"] for i in grain_crop_dictionaries],\n",
    "    grain_indexes=[i for i in grain_crop_dictionaries],\n",
    "    px_to_nms=[grain_crop_dictionaries[i][\"p_to_nm\"] for i in grain_crop_dictionaries],\n",
    "    file_paths=[grain_crop_dictionaries[i][\"file_path\"] for i in grain_crop_dictionaries],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sadly needs to be local to the script because of loading times\n",
    "MODEL_PATH = Path(\"./haribonet_multiclass_improved_norm_big_95_bridging_v1_2024-01-17_10-58-46.h5\")\n",
    "model = load_model(model_path=MODEL_PATH, custom_objects={\"iou\": iou, \"mean_iou\": mean_iou})\n",
    "MODEL_CONFIDENCE = 0.5\n",
    "\n",
    "\n",
    "today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "IMAGE_SAVE_DIR = Path(f\"/Users/sylvi/topo_data/hariborings/extracted_grains/cas9_{SAMPLE}/{today}/\")\n",
    "IMAGE_SAVE_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use automatic crops for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_dicts = {}\n",
    "\n",
    "for grain_index, grain_crop_dict in grain_crop_dictionaries.items():\n",
    "    image = grain_crop_dict[\"cropped_image\"]\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    # plt.show()\n",
    "\n",
    "    p_to_nm = grain_crop_dict[\"p_to_nm\"]\n",
    "\n",
    "    predicted_mask = predict_unet_multiclass(\n",
    "        image=image,\n",
    "        model=model,\n",
    "        confidence=MODEL_CONFIDENCE,\n",
    "        model_image_size=256,\n",
    "        image_output_dir=IMAGE_SAVE_DIR,\n",
    "        filename=\"test\",\n",
    "        image_index=grain_index,\n",
    "        quiet=True,\n",
    "        IMAGE_SAVE_DIR=IMAGE_SAVE_DIR,\n",
    "        normalisation_set_range=(-1, 8),\n",
    "    )\n",
    "\n",
    "    grain_dicts[grain_index] = {\n",
    "        \"image\": image,\n",
    "        \"predicted_mask\": predicted_mask,\n",
    "        \"p_to_nm\": p_to_nm,\n",
    "    }\n",
    "\n",
    "print(f\"Number of images: {len(grain_dicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the crops\n",
    "# for grain_index, grain_dict in grain_dicts.items():\n",
    "#     image = grain_dict[\"image\"]\n",
    "#     predicted_mask = grain_dict[\"predicted_mask\"]\n",
    "#     p_to_nm = grain_dict[\"p_to_nm\"]\n",
    "\n",
    "#     plt.imshow(image, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "#     plt.show()\n",
    "#     plt.imshow(predicted_mask)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get existing crops from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROPPED_IMAGE_DIR = Path(f\"/Users/sylvi/topo_data/hariborings/cas9_crops_p2nm/{SAMPLE}_p2nm\")\n",
    "# assert CROPPED_IMAGE_DIR.exists()\n",
    "# image_files = list(CROPPED_IMAGE_DIR.glob(\"*.npy\"))\n",
    "# image_files = sorted(image_files, key=lambda x: float(re.findall(r\"\\d+\\.\\d+\", x.name)[0]))\n",
    "# print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "# grain_dicts = {}\n",
    "\n",
    "# for index, image_file in enumerate(image_files):\n",
    "#     image = np.load(image_file)\n",
    "#     p_to_nm = float(re.findall(r\"\\d+\\.\\d+\", image_file.name)[0])\n",
    "\n",
    "#     if p_to_nm > MAX_P_TO_NM:\n",
    "#         continue\n",
    "\n",
    "#     predicted_mask = predict_unet_multiclass(\n",
    "#         image=image,\n",
    "#         model=model,\n",
    "#         confidence=MODEL_CONFIDENCE,\n",
    "#         model_image_size=256,\n",
    "#         image_output_dir=IMAGE_SAVE_DIR,\n",
    "#         filename=\"test\",\n",
    "#         image_index=index,\n",
    "#         quiet=True,\n",
    "#         IMAGE_SAVE_DIR=IMAGE_SAVE_DIR,\n",
    "#         normalisation_set_range=(-1, 8),\n",
    "#     )\n",
    "\n",
    "#     grain_dicts[index] = {\n",
    "#         \"image\": image,\n",
    "#         \"predicted_mask\": predicted_mask,\n",
    "#         \"p_to_nm\": p_to_nm,\n",
    "#     }\n",
    "\n",
    "# clear_output()\n",
    "# print(f\"Number of images: {len(grain_dicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(\n",
    "    images: list, masks: list, grain_indexes: list, px_to_nms: list, width=5, VMIN=VMIN, VMAX=VMAX, cmap=CMAP\n",
    "):\n",
    "    num_images = len(images)\n",
    "    num_rows = num_images // width + 1\n",
    "    num_images_in_batch = 2\n",
    "    fig, axes = plt.subplots(num_rows, width * num_images_in_batch, figsize=(width * 4, num_rows * 4))\n",
    "    for i, (image, mask, grain_index, p_to_nm) in enumerate(zip(images, masks, grain_indexes, px_to_nms)):\n",
    "        # Plot image\n",
    "        im_ax = axes[i // width, i % width * num_images_in_batch]\n",
    "        im_ax.imshow(image, cmap=CMAP, vmin=VMIN, vmax=VMAX)\n",
    "        im_ax.set_title(f\"Grain {grain_index} {p_to_nm} p/nm\")\n",
    "        im_ax.axis(\"off\")\n",
    "        # Plot mask\n",
    "        mask_ax = axes[i // width, i % width * num_images_in_batch + 1]\n",
    "        mask_ax.imshow(mask.astype(int))\n",
    "        mask_ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if PLOT_RESULTS:\n",
    "    images = [grain_dicts[i][\"image\"] for i in grain_dicts]\n",
    "    masks = [grain_dicts[i][\"predicted_mask\"] for i in grain_dicts]\n",
    "    grain_indexes = [i for i in grain_dicts]\n",
    "    px_to_nms = [grain_dicts[i][\"p_to_nm\"] for i in grain_dicts]\n",
    "    plot_images(images, masks, grain_indexes, px_to_nms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vet based on numbers of regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ring_and_mask_exists(combined_predicted_mask: np.ndarray):\n",
    "    # Check if there is a ring and gem larger than n pixels in the predicted mask\n",
    "\n",
    "    min_ring_pixels = 40\n",
    "    min_gem_pixels = 40\n",
    "\n",
    "    ring_mask = combined_predicted_mask == 1\n",
    "    gem_mask = combined_predicted_mask == 2\n",
    "    if np.sum(ring_mask) < min_ring_pixels or np.sum(gem_mask) < min_gem_pixels:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def turn_small_gem_regions_into_ring(combined_predicted_mask: np.ndarray):\n",
    "    gem_mask = combined_predicted_mask == 2\n",
    "\n",
    "    # Find largest gem region\n",
    "    gem_labels = label(gem_mask)\n",
    "    gem_regions = regionprops(gem_labels)\n",
    "    gem_areas = [region.area for region in gem_regions]\n",
    "    largest_gem_region = gem_regions[np.argmax(gem_areas)]\n",
    "\n",
    "    # For all other regions, check if they touch a ring region\n",
    "    for region in gem_regions:\n",
    "        if region.label == largest_gem_region.label:\n",
    "            continue\n",
    "        # Get only the pixels in the region\n",
    "        region_mask = gem_labels == region.label\n",
    "        # Dilate the region\n",
    "        small_gem_dilation_strength = 5\n",
    "        dilated_region_mask = region_mask\n",
    "        for i in range(small_gem_dilation_strength):\n",
    "            dilated_region_mask = binary_dilation(dilated_region_mask)\n",
    "        # Get the intersection with the ring mask\n",
    "        intersection = dilated_region_mask & (combined_predicted_mask == 1)\n",
    "        # If there is any intersection, turn the region into a ring\n",
    "        if np.any(intersection):\n",
    "            combined_predicted_mask[dilated_region_mask] = 1\n",
    "\n",
    "    return combined_predicted_mask\n",
    "\n",
    "\n",
    "def remove_all_but_largest_ring_region(combined_predicted_mask: np.ndarray):\n",
    "    ring_mask = combined_predicted_mask == 1\n",
    "    # Find largest region\n",
    "    ring_labels = label(ring_mask)\n",
    "    ring_regions = regionprops(ring_labels)\n",
    "    ring_areas = [region.area for region in ring_regions]\n",
    "    largest_ring_region = ring_regions[np.argmax(ring_areas)]\n",
    "    # For all others, turn to background\n",
    "    for region in ring_regions:\n",
    "        if region.label == largest_ring_region.label:\n",
    "            continue\n",
    "        combined_predicted_mask[ring_labels == region.label] = 0\n",
    "\n",
    "    return combined_predicted_mask\n",
    "\n",
    "\n",
    "def get_number_of_connection_points(combined_predicted_mask: np.ndarray):\n",
    "    ring_mask = combined_predicted_mask == 1\n",
    "    gem_mask = combined_predicted_mask == 2\n",
    "    # Dilate the gem mask\n",
    "    gem_dilation_strength = 1\n",
    "    dilated_gem_mask = gem_mask\n",
    "    for i in range(gem_dilation_strength):\n",
    "        dilated_gem_mask = binary_dilation(dilated_gem_mask)\n",
    "    # Get the intersection with the ring mask\n",
    "    intersection = dilated_gem_mask & ring_mask\n",
    "\n",
    "    # Get number of separate intersection regions\n",
    "    intersection_labels = label(intersection)\n",
    "    intersection_regions = regionprops(intersection_labels)\n",
    "    num_connection_regions = len(intersection_regions)\n",
    "\n",
    "    return num_connection_regions, intersection_labels\n",
    "\n",
    "\n",
    "vetted_grain_dict = {}\n",
    "failed_indexes = []\n",
    "for index, grain_dict in grain_dicts.items():\n",
    "    image = grain_dict[\"image\"]\n",
    "    predicted_mask = grain_dict[\"predicted_mask\"]\n",
    "    p_to_nm = grain_dict[\"p_to_nm\"]\n",
    "\n",
    "    if not check_ring_and_mask_exists(predicted_mask):\n",
    "        failed_indexes.append(index)\n",
    "        continue\n",
    "\n",
    "    predicted_mask = turn_small_gem_regions_into_ring(predicted_mask)\n",
    "\n",
    "    predicted_mask = remove_all_but_largest_ring_region(predicted_mask)\n",
    "\n",
    "    num_connection_regions, intersection_labels = get_number_of_connection_points(predicted_mask)\n",
    "\n",
    "    if num_connection_regions != 2:\n",
    "        failed_indexes.append(index)\n",
    "        continue\n",
    "\n",
    "    vetted_grain_dict[index] = {\n",
    "        \"image\": image,\n",
    "        \"predicted_mask\": predicted_mask,\n",
    "        \"p_to_nm\": p_to_nm,\n",
    "        \"intersection_labels\": intersection_labels,\n",
    "    }\n",
    "\n",
    "print(f\"Number of vetted grains: {len(vetted_grain_dict)}\")\n",
    "\n",
    "if PLOT_RESULTS:\n",
    "    print(f\"Failed indexes: {failed_indexes}\")\n",
    "    failed_images = [grain_dicts[i][\"image\"] for i in failed_indexes]\n",
    "    failed_masks = [grain_dicts[i][\"predicted_mask\"] for i in failed_indexes]\n",
    "    failed_grain_indexes = [i for i in failed_indexes]\n",
    "    failed_px_to_nms = [grain_dicts[i][\"p_to_nm\"] for i in failed_indexes]\n",
    "    plot_images(failed_images, failed_masks, failed_grain_indexes, failed_px_to_nms)\n",
    "\n",
    "if PLOT_RESULTS:\n",
    "    images = [vetted_grain_dict[i][\"image\"] for i in vetted_grain_dict]\n",
    "    masks = [vetted_grain_dict[i][\"predicted_mask\"] for i in vetted_grain_dict]\n",
    "    grain_indexes = [i for i in vetted_grain_dict]\n",
    "    px_to_nms = [vetted_grain_dict[i][\"p_to_nm\"] for i in vetted_grain_dict]\n",
    "    plot_images(images, masks, grain_indexes, px_to_nms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(IMAGE_SAVE_DIR / \"grain_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(vetted_grain_dict, f)\n",
    "#     print(f\"saved grain_dict.pkl to {IMAGE_SAVE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "car-or-truck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
